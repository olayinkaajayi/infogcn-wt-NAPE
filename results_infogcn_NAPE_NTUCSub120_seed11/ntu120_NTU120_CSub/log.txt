[ Thu Mar 21 00:51:16 2024 ] using warm up, epoch: 5
[ Thu Mar 21 00:58:38 2024 ] using warm up, epoch: 5
[ Thu Mar 21 01:02:27 2024 ] Parameters:
{'debug': False, 'log_dir': '.', 'model_saved_name': '', 'noise_ratio': 0.5, 'n_desired': 40000, 'num_point': 25, 'num_person': 2, 'num_class': 120, 'dataset': 'ntu120', 'datacase': 'NTU120_CSub', 'use_vel': False, 'phase': 'train', 'save_score': False, 'avg_best_acc': False, 'seed': 11, 'log_interval': 100, 'save_interval': 1, 'save_epoch': 1, 'eval_interval': 5, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'feeders.feeder_ntu.Feeder', 'num_worker': 4, 'balanced_sampling': False, 'random_rot': True, 'repeat': 1, 'weights': None, 'ignore_weights': [], 'n_heads': 3, 'k': 1, 'z_prior_gain': 3, 'graph': 'graph.ntu_rgb_d.Graph', 'PE_name': 'checkpoint_betaNAPE_ntu-sb.pt', 'model_name': 'infogcn_NAPE_NTUCSub120_seed11', 'model_names_file': 'model_names.txt', 'base_lr': 0.1, 'step': [90, 100], 'optimizer': 'SGD', 'nesterov': True, 'batch_size': 2, 'test_batch_size': 2, 'start_epoch': 0, 'num_epoch': 5, 'weight_decay': 0.0005, 'lr_decay_rate': 0.1, 'warm_up_epoch': 5, 'lambda_1': 0.0001, 'lambda_2': 0.1, 'half': True, 'amp_opt_level': 1, 'count_flop': False, 'flops_filename': 'paper_infogcn_bwd_FLOPs.txt', 'work_dir': 'results_infogcn_NAPE_NTUCSub120_seed11/ntu120_NTU120_CSub'}

[ Thu Mar 21 01:02:27 2024 ] # Parameters: 1570627
[ Thu Mar 21 01:02:27 2024 ] Training epoch: 1
[ Thu Mar 21 02:02:03 2024 ] 	Training loss: 5.3321.  Training acc: 1.01%.
[ Thu Mar 21 02:02:03 2024 ] 	Time consumption: [Data]01%, [Network]99%
[ Thu Mar 21 02:02:03 2024 ] Eval epoch: 1
[ Thu Mar 21 02:17:59 2024 ] 	Mean test loss of 20000 batches: 5.016551.
[ Thu Mar 21 02:17:59 2024 ] 	Top1: 0.54%
[ Thu Mar 21 02:17:59 2024 ] 	Top5: 2.69%
[ Thu Mar 21 02:18:00 2024 ] Training epoch: 2
[ Thu Mar 21 03:15:23 2024 ] 	Training loss: 5.4642.  Training acc: 1.02%.
[ Thu Mar 21 03:15:23 2024 ] 	Time consumption: [Data]01%, [Network]99%
[ Thu Mar 21 03:15:23 2024 ] Eval epoch: 2
[ Thu Mar 21 03:31:28 2024 ] 	Mean test loss of 20000 batches: 5.179863.
[ Thu Mar 21 03:31:28 2024 ] 	Top1: 0.54%
[ Thu Mar 21 03:31:28 2024 ] 	Top5: 3.27%
[ Thu Mar 21 03:31:29 2024 ] Training epoch: 3
[ Thu Mar 21 04:29:11 2024 ] 	Training loss: 5.8036.  Training acc: 0.92%.
[ Thu Mar 21 04:29:11 2024 ] 	Time consumption: [Data]01%, [Network]99%
[ Thu Mar 21 04:29:11 2024 ] Eval epoch: 3
[ Thu Mar 21 04:45:20 2024 ] 	Mean test loss of 20000 batches: 5.256894.
[ Thu Mar 21 04:45:20 2024 ] 	Top1: 1.13%
[ Thu Mar 21 04:45:20 2024 ] 	Top5: 3.29%
[ Thu Mar 21 04:45:22 2024 ] Training epoch: 4
[ Thu Mar 21 05:43:00 2024 ] 	Training loss: 6.1981.  Training acc: 0.97%.
[ Thu Mar 21 05:43:00 2024 ] 	Time consumption: [Data]01%, [Network]99%
[ Thu Mar 21 05:43:00 2024 ] Eval epoch: 4
[ Thu Mar 21 05:59:09 2024 ] 	Mean test loss of 20000 batches: 6.390027.
[ Thu Mar 21 05:59:09 2024 ] 	Top1: 0.54%
[ Thu Mar 21 05:59:09 2024 ] 	Top5: 2.70%
[ Thu Mar 21 05:59:10 2024 ] Training epoch: 5
[ Thu Mar 21 06:56:53 2024 ] 	Training loss: 6.6441.  Training acc: 0.94%.
[ Thu Mar 21 06:56:53 2024 ] 	Time consumption: [Data]01%, [Network]99%
[ Thu Mar 21 06:56:54 2024 ] Eval epoch: 5
[ Thu Mar 21 07:13:00 2024 ] 	Mean test loss of 20000 batches: 5.827783.
[ Thu Mar 21 07:13:01 2024 ] 	Top1: 0.54%
[ Thu Mar 21 07:13:01 2024 ] 	Top5: 3.87%
[ Thu Mar 21 07:29:07 2024 ] Best accuracy: 0.011312306060725088
[ Thu Mar 21 07:29:07 2024 ] Epoch number: 3
[ Thu Mar 21 07:29:07 2024 ] Model name: results_infogcn_NAPE_NTUCSub120_seed11/ntu120_NTU120_CSub
[ Thu Mar 21 07:29:07 2024 ] Model total number of params: 1570627
[ Thu Mar 21 07:29:07 2024 ] Weight decay: 0.0005
[ Thu Mar 21 07:29:07 2024 ] Base LR: 0.1
[ Thu Mar 21 07:29:07 2024 ] Batch Size: 2
[ Thu Mar 21 07:29:07 2024 ] Test Batch Size: 2
[ Thu Mar 21 07:29:07 2024 ] seed: 11
